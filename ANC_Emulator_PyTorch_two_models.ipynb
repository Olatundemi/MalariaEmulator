{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e04000-5dd9-4209-b11f-b87d260a7042",
   "metadata": {},
   "source": [
    "### Parallel Training of EIR and Incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9290e518-54ba-4ec5-9fd6-bdb61699902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363e9114-1527-4131-a7f9-7d3e28a56613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5736dfb-76c5-478f-900e-0ca852d13a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available?  True\n",
      "GPU count:  8\n",
      "GPU Name:  NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Is CUDA available? \", torch.cuda.is_available())\n",
    "print(\"GPU count: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fff231-2e62-4a7f-a29c-9b6fd892ed41",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579e020e-849a-45e5-a733-504e66086a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import process_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eec3ac3-ac5b-4ce5-b74e-3dc2847b8c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>t</th>\n",
       "      <th>prev_true</th>\n",
       "      <th>EIR_true</th>\n",
       "      <th>vol_true</th>\n",
       "      <th>tested</th>\n",
       "      <th>positive</th>\n",
       "      <th>prev_2to10</th>\n",
       "      <th>inc_2to10</th>\n",
       "      <th>incall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.227327</td>\n",
       "      <td>7.614221</td>\n",
       "      <td>0.1</td>\n",
       "      <td>201</td>\n",
       "      <td>49</td>\n",
       "      <td>0.301413</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.001404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0.227322</td>\n",
       "      <td>6.890392</td>\n",
       "      <td>0.1</td>\n",
       "      <td>191</td>\n",
       "      <td>43</td>\n",
       "      <td>0.301406</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0.224282</td>\n",
       "      <td>6.486069</td>\n",
       "      <td>0.1</td>\n",
       "      <td>180</td>\n",
       "      <td>36</td>\n",
       "      <td>0.297466</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>0.220682</td>\n",
       "      <td>6.549618</td>\n",
       "      <td>0.1</td>\n",
       "      <td>227</td>\n",
       "      <td>58</td>\n",
       "      <td>0.292807</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>0.001216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>0.218535</td>\n",
       "      <td>5.842044</td>\n",
       "      <td>0.1</td>\n",
       "      <td>152</td>\n",
       "      <td>41</td>\n",
       "      <td>0.290039</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.001086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run    t  prev_true  EIR_true  vol_true  tested  positive  prev_2to10  \\\n",
       "0    1   30   0.227327  7.614221       0.1     201        49    0.301413   \n",
       "1    1   60   0.227322  6.890392       0.1     191        43    0.301406   \n",
       "2    1   90   0.224282  6.486069       0.1     180        36    0.297466   \n",
       "3    1  120   0.220682  6.549618       0.1     227        58    0.292807   \n",
       "4    1  150   0.218535  5.842044       0.1     152        41    0.290039   \n",
       "\n",
       "   inc_2to10    incall  \n",
       "0   0.002055  0.001404  \n",
       "1   0.001860  0.001270  \n",
       "2   0.001758  0.001200  \n",
       "3   0.001781  0.001216  \n",
       "4   0.001591  0.001086  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and process data\n",
    "df = pd.read_csv(\"data/sim_compendia_train/runs/ANC_Simulation_15000_runs_multi_init_eir.csv\", index_col=0).reset_index(drop=True)\n",
    "#df = process_dataframe(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9210b72-b36a-42da-ad01-6483ccaf7400",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ab50e3-9b8e-4aa0-9c7f-0674f506b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.util import plot_subplots_for_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b736596-b8fc-4d1a-9486-b28f05f59a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_subplots_for_runs(df, num_runs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e7d465-4339-4d0e-9a07-2772ada81d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Correlation\n",
    "#from src.util import plot_cross_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3928e563-3ad6-47f2-be44-1ef33f890a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_cross_correlation(df, 'EIR_true', ['incall'], single_target='incall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce09c4-7a3a-4757-9ad0-0ba2b60b8c09",
   "metadata": {},
   "source": [
    "## Preprocessing - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8fe4a5-5efb-4d30-a0a6-a68ccb49c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "train_data = df[df['run'] <= 14500]\n",
    "eval_data = df[(df['run'] > 14500) & (df['run'] <= 14800)]\n",
    "test_data = df[df['run'] > 14800]\n",
    "cols_to_transform = [\"EIR_true\", \"prev_true\", \"incall\"]\n",
    "log_transform = lambda x: np.log(x + 1e-8)  # Avoids log(0) errors\n",
    "\n",
    "train_data_scaled = train_data.copy()\n",
    "eval_data_scaled = eval_data.copy()\n",
    "test_data_scaled = test_data.copy()\n",
    "for col in cols_to_transform:\n",
    "    train_data_scaled[col] = log_transform(train_data_scaled[col])\n",
    "    eval_data_scaled[col] = log_transform(eval_data_scaled[col])\n",
    "    test_data_scaled[col] = log_transform(test_data_scaled[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36633983-5796-4bdd-89cd-3d5aea076483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.sequence_creator import create_sequences_in_parallel\n",
    "from src.sequence_creator import create_sequences_1output, create_sequences_multi_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3649346d-7739-4f91-98dc-b74b10f1fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "\n",
    "#Creating sequences of tensors\n",
    "X_train_EIR, y_train_EIR = create_sequences_1output(train_data_scaled, window_size)\n",
    "X_eval_EIR, y_eval_EIR = create_sequences_1output(eval_data_scaled, window_size)\n",
    "X_test_EIR, y_test_EIR = create_sequences_1output(test_data_scaled, window_size)\n",
    "\n",
    "X_train_Incidence, y_train_Incidence = create_sequences_multi_inputs(train_data_scaled, window_size, features=['prev_true', 'EIR_true'], target='incall')\n",
    "X_eval_Incidence, y_eval_Incidence = create_sequences_multi_inputs(eval_data_scaled, window_size, features=['prev_true', 'EIR_true'], target='incall')\n",
    "X_test_Incidence, y_test_Incidence = create_sequences_multi_inputs(test_data_scaled, window_size, features=['prev_true', 'EIR_true'], target='incall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d5cc410-8dac-4c07-870d-8c56d4be0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_EIR = torch.utils.data.TensorDataset(X_train_EIR, y_train_EIR)\n",
    "eval_dataset_EIR = torch.utils.data.TensorDataset(X_eval_EIR, y_eval_EIR)\n",
    "test_dataset_EIR = torch.utils.data.TensorDataset(X_test_EIR, y_test_EIR)\n",
    "train_loader_EIR = torch.utils.data.DataLoader(train_dataset_EIR, batch_size=32, shuffle=True)\n",
    "eval_loader_EIR = torch.utils.data.DataLoader(eval_dataset_EIR, batch_size=32, shuffle=False)\n",
    "test_loader_EIR = torch.utils.data.DataLoader(test_dataset_EIR, batch_size=32, shuffle=False)\n",
    "\n",
    "train_dataset_Incidence = torch.utils.data.TensorDataset(X_train_Incidence, y_train_Incidence)\n",
    "eval_dataset_Incidence = torch.utils.data.TensorDataset(X_eval_Incidence, y_eval_Incidence)\n",
    "test_dataset_Incidence = torch.utils.data.TensorDataset(X_test_Incidence, y_test_Incidence)\n",
    "train_loader_Incidence = torch.utils.data.DataLoader(train_dataset_Incidence, batch_size=32, shuffle=True)\n",
    "eval_loader_Incidence = torch.utils.data.DataLoader(eval_dataset_Incidence, batch_size=32, shuffle=False)\n",
    "test_loader_Incidence = torch.utils.data.DataLoader(test_dataset_Incidence, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12b0e14e-7d0f-403c-9aa9-a815688b5729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3538000, 21, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_Incidence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed73f237-5c54-460e-8acd-f365e0bdbf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3538000, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_EIR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d350b0e8-ed2b-47fe-a5be-bf7d6f76cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_EIR(nn.Module):\n",
    "    def __init__(self, input_size, architecture):\n",
    "        super(LSTM_EIR, self).__init__()\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        for i, hidden_size in enumerate(architecture):\n",
    "            self.lstm_layers.append(nn.LSTM(input_size if i == 0 else architecture[i - 1], hidden_size, batch_first=True))\n",
    "        self.fc = nn.Linear(architecture[-1], 1)  # Predicting only EIR\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for lstm in self.lstm_layers:\n",
    "            x, _ = lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "class LSTM_Incidence(nn.Module):\n",
    "    def __init__(self, input_size, architecture):\n",
    "        super(LSTM_Incidence, self).__init__()\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        for i, hidden_size in enumerate(architecture):\n",
    "            self.lstm_layers.append(nn.LSTM(input_size if i == 0 else architecture[i - 1], hidden_size, batch_first=True))\n",
    "        self.fc = nn.Linear(architecture[-1], 1)# Predicting only incidence\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for lstm in self.lstm_layers:\n",
    "            x, _ = lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07a74d2a-e705-40fc-93b1-06f94817c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = {\n",
    "    #\"2_layers\": [128, 64],\n",
    "    \"3_layers\": [200, 100, 50],\n",
    "    \"4_layers\": [256, 128, 64, 32],\n",
    "    #\"5_layers\": [300, 200, 100, 50, 25]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48cc3a2b-d659-4a25-b981-65c1a80ed11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_exp import train_models_in_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f336eb4d-4fcf-4d52-97c3-41ed7d27bafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models: 3_layers\n",
      "Epoch 1/25, EIR Loss: 0.0647, Incidence Loss: 0.14845883, Eval EIR Loss: 0.0318, Eval Incidence Loss: 0.03145280\n",
      "Epoch 2/25, EIR Loss: 0.0287, Incidence Loss: 0.01675272, Eval EIR Loss: 0.0284, Eval Incidence Loss: 0.02476720\n",
      "Epoch 3/25, EIR Loss: 0.0244, Incidence Loss: 0.01151854, Eval EIR Loss: 0.0210, Eval Incidence Loss: 0.00966674\n",
      "Epoch 4/25, EIR Loss: 0.0222, Incidence Loss: 0.00849147, Eval EIR Loss: 0.0237, Eval Incidence Loss: 0.00754984\n",
      "Epoch 5/25, EIR Loss: 0.0210, Incidence Loss: 0.00706027, Eval EIR Loss: 0.0196, Eval Incidence Loss: 0.00576318\n",
      "Epoch 6/25, EIR Loss: 0.0203, Incidence Loss: 0.00598247, Eval EIR Loss: 0.0205, Eval Incidence Loss: 0.00563611\n",
      "Epoch 7/25, EIR Loss: 0.0197, Incidence Loss: 0.00582846, Eval EIR Loss: 0.0195, Eval Incidence Loss: 0.00542092\n",
      "Epoch 8/25, EIR Loss: 0.0199, Incidence Loss: 0.00501644, Eval EIR Loss: 0.0219, Eval Incidence Loss: 0.00790661\n",
      "Epoch 9/25, EIR Loss: 0.0191, Incidence Loss: 0.00466827, Eval EIR Loss: 0.0194, Eval Incidence Loss: 0.00436693\n",
      "Epoch 10/25, EIR Loss: 0.0186, Incidence Loss: 0.00447428, Eval EIR Loss: 0.0199, Eval Incidence Loss: 0.00414587\n",
      "Epoch 11/25, EIR Loss: 0.0183, Incidence Loss: 0.00423033, Eval EIR Loss: 0.0173, Eval Incidence Loss: 0.00416529\n",
      "Epoch 12/25, EIR Loss: 0.0180, Incidence Loss: 0.00431772, Eval EIR Loss: 0.0187, Eval Incidence Loss: 0.00491542\n",
      "Epoch 13/25, EIR Loss: 0.0179, Incidence Loss: 0.00475912, Eval EIR Loss: 0.0183, Eval Incidence Loss: 0.00447338\n",
      "Epoch 14/25, EIR Loss: 0.0178, Incidence Loss: 0.00381695, Eval EIR Loss: 0.0174, Eval Incidence Loss: 0.00403568\n",
      "Epoch 15/25, EIR Loss: 0.0180, Incidence Loss: 0.00395367, Eval EIR Loss: 0.0216, Eval Incidence Loss: 0.00373151\n",
      "Epoch 16/25, EIR Loss: 0.0175, Incidence Loss: 0.00359829, Eval EIR Loss: 0.0179, Eval Incidence Loss: 0.00520445\n",
      "Epoch 17/25, EIR Loss: 0.0173, Incidence Loss: 0.00353118, Eval EIR Loss: 0.0169, Eval Incidence Loss: 0.00381664\n",
      "Epoch 18/25, EIR Loss: 0.0174, Incidence Loss: 0.00336380, Eval EIR Loss: 0.0173, Eval Incidence Loss: 0.00377558\n",
      "Epoch 19/25, EIR Loss: 0.0173, Incidence Loss: 0.00342680, Eval EIR Loss: 0.0184, Eval Incidence Loss: 0.00393043\n",
      "Epoch 20/25, EIR Loss: 0.0173, Incidence Loss: 0.00337735, Eval EIR Loss: 0.0166, Eval Incidence Loss: 0.00376000\n",
      "Epoch 21/25, EIR Loss: 0.0170, Incidence Loss: 0.00364532, Eval EIR Loss: 0.0201, Eval Incidence Loss: 0.00463593\n",
      "Epoch 22/25, EIR Loss: 0.0175, Incidence Loss: 0.00316220, Eval EIR Loss: 0.0185, Eval Incidence Loss: 0.01217710\n",
      "Epoch 23/25, EIR Loss: 0.0169, Incidence Loss: 0.00326429, Eval EIR Loss: 0.0177, Eval Incidence Loss: 0.00390703\n",
      "Epoch 24/25, EIR Loss: 0.0168, Incidence Loss: 0.21909284, Eval EIR Loss: 0.0177, Eval Incidence Loss: 0.02027646\n",
      "Epoch 25/25, EIR Loss: 0.0168, Incidence Loss: 0.00594830, Eval EIR Loss: 0.0183, Eval Incidence Loss: 0.00398571\n",
      "Models saved as src/LSTM_EIR_3_layers.pth and src/LSTM_Incidence_3_layers.pth\n",
      "Training models: 4_layers\n",
      "Epoch 1/25, EIR Loss: 0.0617, Incidence Loss: 0.33722402, Eval EIR Loss: 0.0294, Eval Incidence Loss: 0.03487849\n",
      "Epoch 2/25, EIR Loss: 0.0285, Incidence Loss: 0.01808298, Eval EIR Loss: 0.0250, Eval Incidence Loss: 0.01407407\n",
      "Epoch 3/25, EIR Loss: 0.0242, Incidence Loss: 0.01263218, Eval EIR Loss: 0.0217, Eval Incidence Loss: 0.01057877\n",
      "Epoch 4/25, EIR Loss: 0.0221, Incidence Loss: 0.00989343, Eval EIR Loss: 0.0197, Eval Incidence Loss: 0.00980128\n",
      "Epoch 5/25, EIR Loss: 0.0209, Incidence Loss: 0.00818723, Eval EIR Loss: 0.0197, Eval Incidence Loss: 0.00632530\n",
      "Epoch 6/25, EIR Loss: 0.0202, Incidence Loss: 0.00682195, Eval EIR Loss: 0.0186, Eval Incidence Loss: 0.00625172\n",
      "Epoch 7/25, EIR Loss: 0.0194, Incidence Loss: 0.00597747, Eval EIR Loss: 0.0225, Eval Incidence Loss: 0.00626561\n",
      "Epoch 8/25, EIR Loss: 0.0189, Incidence Loss: 0.00539265, Eval EIR Loss: 0.0176, Eval Incidence Loss: 0.00596384\n",
      "Epoch 9/25, EIR Loss: 0.0186, Incidence Loss: 0.00486324, Eval EIR Loss: 0.0181, Eval Incidence Loss: 0.00467477\n",
      "Epoch 10/25, EIR Loss: 0.0184, Incidence Loss: 0.00456500, Eval EIR Loss: 0.0190, Eval Incidence Loss: 0.00500051\n",
      "Epoch 11/25, EIR Loss: 0.0195, Incidence Loss: 0.00447204, Eval EIR Loss: 0.0185, Eval Incidence Loss: 0.00521919\n",
      "Epoch 12/25, EIR Loss: 0.0179, Incidence Loss: 0.00398402, Eval EIR Loss: 0.0182, Eval Incidence Loss: 0.00440412\n",
      "Epoch 13/25, EIR Loss: 0.0178, Incidence Loss: 0.00493951, Eval EIR Loss: 0.0205, Eval Incidence Loss: 0.00414021\n",
      "Epoch 14/25, EIR Loss: 0.0176, Incidence Loss: 0.00371294, Eval EIR Loss: 0.0179, Eval Incidence Loss: 0.00441579\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name, architecture in architectures.items():\n",
    "    print(f\"Training models: {name}\")\n",
    "    model_EIR = LSTM_EIR(input_size=1, architecture=architecture)\n",
    "    model_Incidence = LSTM_Incidence(input_size=2, architecture=architecture)\n",
    "    trained_EIR, trained_Incidence, loss_history_EIR, loss_history_Incidence, eval_loss_history_EIR, eval_loss_history_Incidence, _ = train_models_in_parallel(\n",
    "        model_EIR, model_Incidence,\n",
    "        train_loader_EIR, train_loader_Incidence,\n",
    "        eval_loader_EIR, eval_loader_Incidence,\n",
    "        model_name=name, epochs=25, lr=0.001\n",
    "    )\n",
    "    results.append({\n",
    "        \"name\": name,\n",
    "        \"model_EIR\": trained_EIR,\n",
    "        \"model_Incidence\": trained_Incidence,\n",
    "        \"loss_history_EIR\": loss_history_EIR,\n",
    "        \"loss_history_Incidence\": loss_history_Incidence,\n",
    "        \"eval_loss_history_EIR\": eval_loss_history_EIR,\n",
    "        \"eval_loss_history_Incidence\": eval_loss_history_Incidence\n",
    "    })\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914961a0-8824-42e9-9674-10f0776a43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {'MSE': mse, 'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Only unsqueeze if needed\n",
    "            if X.dim() == 2:\n",
    "                X = X.unsqueeze(-1)\n",
    "            outputs = model(X)\n",
    "            preds.append(outputs.cpu().numpy().flatten())\n",
    "            targets.append(y.cpu().numpy().flatten())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "    return compute_metrics(targets, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6d5b3-474f-4153-adf2-db4977361c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Collect metrics across all models for both EIR and Incidence\n",
    "all_metrics = []\n",
    "\n",
    "for res in results:\n",
    "    model_name = res[\"name\"]\n",
    "    \n",
    "    # EIR metrics\n",
    "    metrics_train_eir = evaluate_model(res[\"model_EIR\"], train_loader_EIR, device)\n",
    "    metrics_eval_eir = evaluate_model(res[\"model_EIR\"], eval_loader_EIR, device)\n",
    "    metrics_test_eir = evaluate_model(res[\"model_EIR\"], test_loader_EIR, device)\n",
    "    \n",
    "    # Incidence metrics\n",
    "    metrics_train_inc = evaluate_model(res[\"model_Incidence\"], train_loader_Incidence, device)\n",
    "    metrics_eval_inc = evaluate_model(res[\"model_Incidence\"], eval_loader_Incidence, device)\n",
    "    metrics_test_inc = evaluate_model(res[\"model_Incidence\"], test_loader_Incidence, device)\n",
    "    \n",
    "    # Append to list for DataFrame creation\n",
    "    for dataset_name, metrics, target in [\n",
    "        (\"Train\", metrics_train_eir, \"EIR\"),\n",
    "        (\"Eval\", metrics_eval_eir, \"EIR\"),\n",
    "        (\"Test\", metrics_test_eir, \"EIR\"),\n",
    "        (\"Train\", metrics_train_inc, \"Incidence\"),\n",
    "        (\"Eval\", metrics_eval_inc, \"Incidence\"),\n",
    "        (\"Test\", metrics_test_inc, \"Incidence\"),\n",
    "    ]:\n",
    "        all_metrics.append({\n",
    "            \"Architecture\": model_name,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Target\": target,\n",
    "            \"MSE\": metrics[\"MSE\"],\n",
    "            \"RMSE\": metrics[\"RMSE\"],\n",
    "            \"MAE\": metrics[\"MAE\"],\n",
    "            \"R2\": metrics[\"R2\"]\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "df_metrics = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Define plot function\n",
    "def plot_metrics_combined(df_metrics):\n",
    "    metrics_to_plot = [\"MSE\", \"RMSE\", \"MAE\", \"R2\"]\n",
    "    n_metrics = len(metrics_to_plot)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_metrics, figsize=(5 * n_metrics, 10))\n",
    "    fig.suptitle(\"Model Performance Metrics by Architecture and Dataset\", fontsize=16, y=1.02)\n",
    "\n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        for j, target in enumerate([\"EIR\", \"Incidence\"]):\n",
    "            ax = axes[j, i]\n",
    "            data = df_metrics[df_metrics[\"Target\"] == target]\n",
    "            sns.barplot(\n",
    "                data=data,\n",
    "                x=\"Architecture\", y=metric, hue=\"Dataset\",\n",
    "                ax=ax, palette=\"Set2\"\n",
    "            )\n",
    "            ax.set_title(f\"{target} - {metric}\")\n",
    "            ax.set_xlabel(\"Architecture\")\n",
    "            ax.set_ylabel(metric)\n",
    "            ax.legend(title=\"Dataset\")\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot all metrics\n",
    "plot_metrics_combined(df_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0fab3f-ef19-4f1b-bae5-875497928096",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "metrics_to_plot = ['MSE', 'MAE', 'RMSE', 'R2']\n",
    "palette = sns.color_palette(\"tab10\")\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    g = sns.catplot(\n",
    "    data=df_metrics, kind=\"bar\",\n",
    "    x=\"Architecture\", y=metric, hue=\"Dataset\",\n",
    "    col=\"Target\", palette=palette,\n",
    "    height=5, aspect=1, legend_out=False\n",
    ")\n",
    "    \n",
    "    g.set_axis_labels(\"Architecture\", metric)\n",
    "    g.set_titles(\"{col_name}\")\n",
    "    g._legend.set_title(\"Dataset Split\")\n",
    "    \n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    g.fig.suptitle(f\"{metric} Comparison Across Models (EIR & Incidence)\", fontsize=16)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7724414-f234-499f-98f0-4d4acd3bd463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
